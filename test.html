<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Logging Template</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 20px auto;
            padding: 0 20px;
        }

        #title {
            background: linear-gradient(to right, #2c3e50, #3498db);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        #logging {
            margin-top: 20px;
            padding: 15px;
            border: 1px solid #ddd;
            border-radius: 8px;
            background-color: #f9f9f9;
            min-height: 200px;
            max-height: 400px;
            overflow-y: auto;
        }

        .log-entry {
            margin: 5px 0;
            padding: 5px;
            border-bottom: 1px solid #eee;
        }

        .timestamp {
            color: #666;
            font-size: 0.9em;
            margin-right: 10px;
        }
    </style>
</head>
<body>
    <div id="title">
        <h1>JS-PyTorch Web Demo</h1>
    </div>
    
    <div id="logging"></div>
    <!-- import js-pytorch -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/js-pytorch/0.7.2/js-pytorch-browser.js" 
    integrity="sha512-l22t7GnqXvHBMCBvPUBdFO2TEYxnb1ziCGcDQcpTB2un16IPA4FE5SIZ8bUR+RwoDZGikQkWisO+fhnakXt9rg==" 
    crossorigin="anonymous" 
    referrerpolicy="no-referrer"></script>
    <script>
// Implement Module class:
class NeuralNet extends nn.Module {
    constructor(in_size, hidden_size, out_size) {
      super();
      // Instantiate Neural Network's Layers:
      this.w1 = new nn.Linear(in_size, hidden_size);
      this.relu1 = new nn.ReLU();
      this.w2 = new nn.Linear(hidden_size, hidden_size);
      this.relu2 = new nn.ReLU();
      this.w3 = new nn.Linear(hidden_size, out_size);
    };
  
    forward(x) {
      let z;
      z = this.w1.forward(x);
      z = this.relu1.forward(z);
      z = this.w2.forward(z);
      z = this.relu2.forward(z);
      z = this.w3.forward(z);
      return z;
    };
  };
  
  // Instantiate Model:
  let in_size = 16;
  let hidden_size = 32;
  let out_size = 10;
  let batch_size = 16;
  
  let model = new NeuralNet(in_size,hidden_size,out_size);
  
  // Define loss function and optimizer:
  let loss_func = new nn.CrossEntropyLoss();
  let optimizer = new optim.Adam(model.parameters(), 3e-3);
  
  // Instantiate input and output:
  let x = torch.randn([batch_size, in_size]);
  let y = torch.randint(0, out_size, [batch_size]);
  let loss;
  
  // Training Loop:
  for (let i = 0; i < 256; i++) {
    let z = model.forward(x);
  
    // Get loss:
    loss = loss_func.forward(z, y);
  
    // Backpropagate the loss using torch.tensor's backward() method:
    loss.backward();
  
    // Update the weights:
    optimizer.step();
  
    // Reset the gradients to zero after each training step:
    optimizer.zero_grad();
  
    // Print current loss:
    console.log(`Iter: ${i} - Loss: ${loss.data}`);
  }
    </script>
</body>
</html>