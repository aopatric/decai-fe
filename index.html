<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JS-PyTorch Web Demo</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 20px auto;
            padding: 0 20px;
        }

        #title {
            background: linear-gradient(to right, #2c3e50, #3498db);
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        #log {
            background-color: #fff;
            border: 1px solid #ccc;
            padding: 10px;
            border-radius: 5px;
            height: 300px;
            overflow-y: auto;
            font-family: monospace;
            white-space: pre-wrap;
        }

        .log-entry {
            margin: 5px 0;
            padding: 5px;
            border-bottom: 1px solid #eee;
        }

        .log-info { color: #0066cc; }
        .log-error { color: #cc0000; }
    </style>
</head>
<body>
    <div id="title">
        <h1>JS-PyTorch Web Demo</h1>
    </div>
    
    <div id="log"></div>
    <!-- import js-pytorch -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/js-pytorch/0.7.2/js-pytorch-browser.js" 
    integrity="sha512-l22t7GnqXvHBMCBvPUBdFO2TEYxnb1ziCGcDQcpTB2un16IPA4FE5SIZ8bUR+RwoDZGikQkWisO+fhnakXt9rg==" 
    crossorigin="anonymous" 
    referrerpolicy="no-referrer"></script>
    <script>
        // Updated logging helper
        function log_in_div(message, type = 'info') {
            const logDiv = document.getElementById('log');
            const entry = document.createElement('div');
            entry.className = `log-entry log-${type}`;
            entry.textContent = `${new Date().toISOString()} - ${message}`;
            logDiv.appendChild(entry);
            logDiv.scrollTop = logDiv.scrollHeight;

            if (type === 'info') {
                console.log(message);
            } else {
                console.error(message);
            }
        }

        // Classification Model Definition
        class BloodMNISTNet extends nn.Module {
            constructor(in_size, hidden_size, out_size) {
            super();
            log_in_div(`Creating new BloodMNISTNet with input size ${in_size}, hidden size ${hidden_size}, and output size ${out_size}...`);
            // Instantiate Neural Network's Layers:
            this.w1 = new nn.Linear(in_size, hidden_size);
            this.relu1 = new nn.ReLU();
            this.w2 = new nn.Linear(hidden_size, hidden_size);
            this.relu2 = new nn.ReLU();
            this.w3 = new nn.Linear(hidden_size, out_size);
            };
        
            forward(x) {
            let z;
            z = this.w1.forward(x);
            z = this.relu1.forward(z);
            z = this.w2.forward(z);
            z = this.relu2.forward(z);
            z = this.w3.forward(z);
            return z;
            };
        };

        class ModelTrainer {
            constructor(in_size, hidden_size, out_size, batch_size) {
                log_in_div(`building test constructor...`);
                this.in_size = in_size;
                this.hidden_size = hidden_size;
                this.out_size = out_size;
                this.batch_size = batch_size;
            }
            train(train_x, train_y) {
                let model = new BloodMNISTNet(this.in_size, this.hidden_size, this.out_size);
                let loss_fn = new nn.CrossEntropyLoss();
                let optimizer = new optim.Adam(model.parameters(), 3e-3);
                
                const num_samples = train_x.shape[0];
                const num_batches = Math.floor(num_samples / this.batch_size);
                const epochs = 3; // You can adjust this

                log_in_div(`Training with ${num_samples} samples in ${num_batches} batches for ${epochs} epochs`);
                
                for (let epoch = 0; epoch < epochs; epoch++) {
                    let epoch_loss = 0;
                    
                    for (let i = 0; i < num_batches; i++) {
                        const start_idx = i * this.batch_size;
                        const end_idx = start_idx + this.batch_size;
                        
                        // Get batch
                        let batch_x_data = train_x.data.slice(start_idx, end_idx);
                        let batch_x = torch.tensor(batch_x_data);

                        let batch_y_data = train_y.data.slice(start_idx, end_idx);
                        let batch_y = torch.tensor(batch_y_data);
                        
                        // Forward pass
                        let z = model.forward(batch_x);
                        let loss = loss_fn.forward(z, batch_y);
                        epoch_loss += loss.data;
                        
                        // Backward pass
                        loss.backward();
                        optimizer.step();
                        optimizer.zero_grad();
                        
                        if (i % 10 === 0) {
                            log_in_div(`Epoch ${epoch + 1}/${epochs}, Batch ${i}/${num_batches}, Loss: ${loss.data}`);
                        }
                    }
                    
                    const avg_loss = epoch_loss / num_batches;
                    log_in_div(`Epoch ${epoch + 1} complete. Average loss: ${avg_loss}`, 'info');
                }
            }

        }

        // helper to load the data from the jsons
        async function load_data() {
            try {
                log_in_div(`loading data...`);
                let train_response = await fetch('public/data/bloodmnist_train.json');
                
                if (!train_response.ok) {
                    throw new Error(`HTTP error! status: ${train_response.status}`);
                }
                
                log_in_div(`fetching completed. Starting JSON parsing...`);
                const text = await train_response.text();
                const train_data = JSON.parse(text);
                
                // Limit to first 1000 samples for testing
                const MAX_SAMPLES = 1000;
                const limited_data = train_data.slice(0, MAX_SAMPLES);
                log_in_div(`Using ${limited_data.length} samples for training...`);

                const x_data = [];
                const y_data = [];
                
                // Process in smaller chunks
                const chunkSize = 50;
                for (let i = 0; i < limited_data.length; i += chunkSize) {
                    const chunk = limited_data.slice(i, i + chunkSize);
                    x_data.push(...chunk.map(item => item.image));
                    y_data.push(...chunk.map(item => item.label));
                    
                    log_in_div(`Processed ${i + chunk.length}/${limited_data.length} samples...`);
                    // Allow UI to update
                    await new Promise(resolve => setTimeout(resolve, 0));
                }

                log_in_div(`formatting data as tensors...`);
                const x_tensor = torch.tensor(x_data);
                const y_tensor = torch.tensor(y_data);

                return [x_tensor, y_tensor];
            } catch (error) {
                log_in_div(`Error loading data: ${error.message}`, 'error');
                throw error;
            }
        }
    
    
        // load data and run an experiment
        async function run_experiment() {
            let [train_x, train_y] = await load_data();
            let trainer = new ModelTrainer(2352, 512, 8, 1);
            trainer.train(train_x, train_y);
        }

        run_experiment();
    </script>
</body>
</html>